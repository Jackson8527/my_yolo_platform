import os
import threading
import subprocess
import shutil
import zipfile
import sys
import json
import yaml
import time
import pandas as pd
from config import Config

class TrainingState:
    def __init__(self):
        self.is_training = False
        self.logs = []
        self.process = None
        self.stop_event = False 

state = TrainingState()

# ================= COCO æ ¼å¼è½¬æ¢å™¨ =================
class COCOConverter:
    @staticmethod
    def convert(dataset_root):
        """ è‡ªåŠ¨æ£€æµ‹å¹¶è½¬æ¢ COCO æ ¼å¼ """
        train_json, val_json = None, None
        img_train_dir, img_val_dir = None, None
        
        # 1. æ‰«ææ–‡ä»¶
        for root, dirs, files in os.walk(dataset_root):
            if 'train2017.json' in files: train_json = os.path.join(root, 'train2017.json')
            if 'val2017.json' in files: val_json = os.path.join(root, 'val2017.json')
            if 'train2017' in dirs: img_train_dir = os.path.join(root, 'train2017')
            if 'val2017' in dirs: img_val_dir = os.path.join(root, 'val2017')

        # å¦‚æœæ‰¾ä¸åˆ° jsonï¼Œå°è¯•ç›´æ¥æ‰¾ data.yaml
        if not train_json:
            for root, _, files in os.walk(dataset_root):
                if 'data.yaml' in files: return os.path.join(root, 'data.yaml')
            raise Exception("æœªæ‰¾åˆ° train2017.json æˆ– data.yaml")

        state.logs.append(f"æ£€æµ‹åˆ° COCO æ ¼å¼ï¼Œæ­£åœ¨è½¬æ¢...\n")
        
        # 2. åˆ›å»ºç›®å½•
        output_dir = os.path.join(dataset_root, 'yolo_formatted')
        for split in ['train', 'val']:
            os.makedirs(f"{output_dir}/images/{split}", exist_ok=True)
            os.makedirs(f"{output_dir}/labels/{split}", exist_ok=True)

        # 3. è½¬æ¢
        names = COCOConverter._process_json(train_json, img_train_dir, output_dir, 'train')
        if val_json and img_val_dir:
            COCOConverter._process_json(val_json, img_val_dir, output_dir, 'val')

        # 4. ç”Ÿæˆ yaml
        yaml_content = {
            'path': output_dir,
            'train': 'images/train',
            'val': 'images/val',
            'nc': len(names),
            'names': names
        }
        yaml_path = os.path.join(output_dir, 'data.yaml')
        with open(yaml_path, 'w') as f:
            yaml.dump(yaml_content, f, sort_keys=False)
            
        state.logs.append(f"âœ… è½¬æ¢å®Œæˆï¼ç±»åˆ«: {names}\n")
        return yaml_path

    @staticmethod
    def _process_json(json_path, img_source, output_base, split):
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        cat_map = {cat['id']: i for i, cat in enumerate(data['categories'])}
        names = [cat['name'] for cat in data['categories']]
        images_info = {img['id']: img for img in data['images']}
        
        # å¤åˆ¶å›¾ç‰‡
        for img_id, info in images_info.items():
            src = os.path.join(img_source, info['file_name'])
            dst = os.path.join(output_base, 'images', split, info['file_name'])
            if os.path.exists(src): shutil.copy(src, dst)

        # ç”Ÿæˆæ ‡æ³¨
        for ann in data['annotations']:
            img = images_info.get(ann['image_id'])
            if not img: continue
            
            x, y, w, h = ann['bbox']
            # å½’ä¸€åŒ– xywh
            dw = 1. / img['width']
            dh = 1. / img['height']
            x_center = (x + w / 2.0) * dw
            y_center = (y + h / 2.0) * dh
            w = w * dw
            h = h * dh
            
            cls_id = cat_map[ann['category_id']]
            txt_name = os.path.splitext(img['file_name'])[0] + ".txt"
            txt_path = os.path.join(output_base, 'labels', split, txt_name)
            
            with open(txt_path, 'a') as f:
                f.write(f"{cls_id} {x_center} {y_center} {w} {h}\n")
        
        return names

# ================= æ•°æ®è¯»å–é€»è¾‘ =================

def get_training_metrics(project_name):
    """ è¯»å– results.csv è¿”å›æ‰€æœ‰æ•°æ®ç”¨äºç”»å›¾ """
    csv_path = os.path.join(Config.RUNS_FOLDER, project_name, 'results.csv')
    if not os.path.exists(csv_path): return None
    try:
        df = pd.read_csv(csv_path)
        df.columns = [c.strip() for c in df.columns] # å»ç©ºæ ¼
        return {
            "epoch": df['epoch'].tolist(),
            "box_loss": df['train/box_loss'].tolist(),
            "map50": df['metrics/mAP50(B)'].tolist()
        }
    except: return None

def get_latest_metrics(project_name):
    """ è¯»å–æœ€åä¸€è¡Œæ•°æ®ç”¨äºè¿›åº¦æ¡ """
    csv_path = os.path.join(Config.RUNS_FOLDER, project_name, 'results.csv')
    if not os.path.exists(csv_path): return None
    try:
        df = pd.read_csv(csv_path)
        df.columns = [c.strip() for c in df.columns]
        if df.empty: return None
        
        last = df.iloc[-1]
        return {
            "epoch": int(last['epoch']),
            "box_loss": round(last['train/box_loss'], 5),
            "cls_loss": round(last.get('train/cls_loss', 0), 5),
            "map50": round(last['metrics/mAP50(B)'], 3)
        }
    except: return None

# ================= è®­ç»ƒçº¿ç¨‹é€»è¾‘ =================

def _run_full_process_thread(zip_path, dataset_name, model_name, epochs, batch, imgsz, project_name, extra_args):
    global state
    state.stop_event = False
    
    try:
        # === 1. åˆ¤æ–­æ˜¯å¦ä¸ºæ¢å¤è®­ç»ƒ (Resume) ===
        is_resume = extra_args.get('resume') == 'True'
        resume_path = os.path.join(Config.RUNS_FOLDER, project_name, 'weights', 'last.pt')
        
        yaml_path = None # åˆå§‹åŒ–

        if is_resume:
            if not os.path.exists(resume_path):
                state.logs.append(f"âŒ æ— æ³•æ¢å¤è®­ç»ƒï¼šæœªæ‰¾åˆ° {resume_path}\n")
                state.is_training = False
                return
            state.logs.append(f"ğŸ”„ [1/3] æ£€æµ‹åˆ°æ¢å¤è®­ç»ƒè¯·æ±‚ï¼ŒåŠ è½½: {resume_path}...\n")
            # æ¢å¤è®­ç»ƒæ—¶ï¼Œä¸éœ€è¦è§£å‹æ•°æ®é›†ï¼ˆå‡è®¾å·²ç»å­˜åœ¨ï¼‰ï¼Œç›´æ¥å¤ç”¨
            # ä½†ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬è¿˜æ˜¯å®šä¹‰ä¸€ä¸‹ yaml è·¯å¾„ï¼Œé˜²æ­¢ yolo æ‰¾ä¸åˆ°
            # è¿™é‡Œç®€å•å¤„ç†ï¼šå‡è®¾ç”¨æˆ·ä¹‹å‰çš„è·¯å¾„æ²¡å˜ã€‚
            # å®é™…ä¸Š resume=True æ—¶ï¼ŒYOLO ä¼šä» last.pt é‡Œè¯»å–æ‰€æœ‰é…ç½®ï¼Œæˆ‘ä»¬å¯ä»¥è·³è¿‡è§£å‹æ­¥éª¤
        else:
            # === éæ¢å¤è®­ç»ƒï¼šæ­£å¸¸è§£å‹å’Œè½¬æ¢ ===
            extract_path = os.path.join(Config.DATASET_FOLDER, dataset_name)
            state.logs.append(f"ğŸ“¦ [1/3] è§£å‹æ•°æ®é›†: {dataset_name}...\n")
            
            if os.path.exists(extract_path): shutil.rmtree(extract_path)
            os.makedirs(extract_path)
            
            with zipfile.ZipFile(zip_path, 'r') as z: z.extractall(extract_path)
            if state.stop_event: raise Exception("ä»»åŠ¡è¢«ç»ˆæ­¢")

            state.logs.append(f"ğŸ”„ [2/3] æ£€æŸ¥æ ¼å¼...\n")
            try:
                # å‡è®¾ä½ ä¿ç•™äº† COCOConverter
                from services.training_service import COCOConverter
                yaml_path = COCOConverter.convert(extract_path)
            except Exception as e:
                # å…œåº•å¯»æ‰¾
                found = False
                for r, _, f in os.walk(extract_path):
                    if 'data.yaml' in f:
                        yaml_path = os.path.join(r, 'data.yaml')
                        found = True
                        break
                if not found: raise Exception("æ‰¾ä¸åˆ° data.yaml ä¸”æ— æ³•è‡ªåŠ¨è½¬æ¢")
            
            state.logs.append(f"âœ… æ•°æ®é›†å‡†å¤‡å°±ç»ª: {yaml_path}\n")

        if state.stop_event: raise Exception("ä»»åŠ¡è¢«ç»ˆæ­¢")

        # === 3. æ„é€ è®­ç»ƒå‘½ä»¤ ===
        state.logs.append(f"ğŸš€ [3/3] å¯åŠ¨è®­ç»ƒ...\n")
        
        # å¯»æ‰¾ yolo æ‰§è¡Œè·¯å¾„
        yolo_exe = os.path.join(os.path.dirname(sys.executable), 'yolo')
        if not os.path.exists(yolo_exe): yolo_exe = 'yolo'
        if os.name == 'nt':
            win_exe = os.path.join(os.path.dirname(sys.executable), 'Scripts', 'yolo.exe')
            if os.path.exists(win_exe): yolo_exe = win_exe

        cmd = [yolo_exe, "train"]

        if is_resume:
            # æ¢å¤è®­ç»ƒæ ¸å¿ƒå‚æ•°
            cmd.append(f"model={resume_path}")
            cmd.append("resume=True")
        else:
            # æ–°è®­ç»ƒæ ¸å¿ƒå‚æ•°
            cmd.append(f"model={model_name}")
            cmd.append(f"data={yaml_path}")
            cmd.append(f"epochs={epochs}")
            cmd.append(f"batch={batch}")
            cmd.append(f"imgsz={imgsz}")
            cmd.append(f"project={Config.RUNS_FOLDER}")
            cmd.append(f"name={project_name}")
            cmd.append("exist_ok=True")
            
            # === æ·»åŠ å¢å¼ºå‚æ•° (Augmentation) ===
            # åªæœ‰åœ¨æ–°è®­ç»ƒæ—¶ç”Ÿæ•ˆï¼Œæ¢å¤è®­ç»ƒä¼šæ²¿ç”¨ä¹‹å‰çš„è®¾ç½®
            aug_params = ['degrees', 'translate', 'scale', 'shear', 'perspective', 'flipud', 'fliplr', 'mosaic', 'mixup']
            for arg in aug_params:
                if extra_args.get(arg):
                    cmd.append(f"{arg}={extra_args.get(arg)}")

            # æ·»åŠ ç³»ç»Ÿå‚æ•°
            sys_params = ['device', 'workers', 'patience', 'optimizer', 'cos_lr']
            for arg in sys_params:
                if extra_args.get(arg):
                    cmd.append(f"{arg}={extra_args.get(arg)}")

        state.logs.append(f"ğŸ”§ å‘½ä»¤: {' '.join(cmd)}\n")
        
        process = subprocess.Popen(
            cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, 
            text=True, bufsize=1, encoding='utf-8', errors='replace'
        )
        state.process = process

        for line in iter(process.stdout.readline, ''):
            if state.stop_event:
                process.terminate()
                state.logs.append("\nğŸ›‘ ç”¨æˆ·ç‚¹å‡»ç»ˆæ­¢ï¼Œæ­£åœ¨åœæ­¢...\n")
                break
            if line: state.logs.append(line)
        
        if not state.stop_event:
            if process.wait() == 0: state.logs.append("\nâœ… è®­ç»ƒå®Œæˆï¼\n")
            else: state.logs.append("\nâŒ è®­ç»ƒå¼‚å¸¸é€€å‡º\n")

    except Exception as e:
        state.logs.append(f"\nâŒ é”™è¯¯: {str(e)}\n")
    finally:
        state.is_training = False
        state.process = None

def start_training_task(file, model_name, epochs, batch, imgsz, project_name, extra_args):
    if state.is_training: raise Exception("å·²æœ‰ä»»åŠ¡åœ¨è¿è¡Œ")
    
    # å¦‚æœæ˜¯ Resumeï¼Œä¸éœ€è¦ä¸Šä¼ æ–‡ä»¶
    is_resume = extra_args.get('resume') == 'True'
    
    zip_path = ""
    dataset_name = ""

    if not is_resume:
        if not file: raise Exception("æ–°è®­ç»ƒå¿…é¡»ä¸Šä¼ æ•°æ®é›†")
        filename = file.filename
        zip_path = os.path.join(Config.UPLOAD_FOLDER, filename)
        file.save(zip_path)
        dataset_name = os.path.splitext(filename)[0]
    
    state.logs = [f"--- å¼€å§‹ä»»åŠ¡: {project_name} {'(æ¢å¤è®­ç»ƒ)' if is_resume else ''} ---\n"]
    state.is_training = True
    
    thread = threading.Thread(
        target=_run_full_process_thread,
        args=(zip_path, dataset_name, model_name, epochs, batch, imgsz, project_name, extra_args)
    )
    thread.daemon = True
    thread.start()

def stop_training():
    if state.is_training:
        state.stop_event = True
        if state.process: state.process.terminate()
        return True
    return False

def get_logs():
    return "".join(state.logs), state.is_training